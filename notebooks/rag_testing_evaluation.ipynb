{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Testing & Evaluation with Groq\n",
    "\n",
    "This notebook provides comprehensive testing and evaluation of the RAG pipeline including:\n",
    "- Vector store functionality testing\n",
    "- Similarity search evaluation  \n",
    "- Complete RAG pipeline performance analysis\n",
    "- Parameter optimization\n",
    "- Quality assessment\n",
    "\n",
    "**Uses Groq API for LLM generation and your actual data directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "# Core imports\n",
    "from src.rag_pipeline import RAGPipeline\n",
    "from src.vector_store import VectorStore\n",
    "from src.document_processor import DocumentProcessor\n",
    "\n",
    "# Analysis imports\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Data directory exists: {os.path.exists('./data')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize RAG Pipeline with Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG pipeline with Groq\n",
    "print(\"Initializing RAG Pipeline with Groq...\")\n",
    "\n",
    "try:\n",
    "    rag_pipeline = RAGPipeline(\n",
    "        model_name=\"llama-3.1-8b-instant\",  # Using Groq's fast model\n",
    "        max_chunks=5,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"RAG Pipeline initialized successfully!\")\n",
    "    print(f\"Model: {rag_pipeline.model_name}\")\n",
    "    print(f\"Max chunks: {rag_pipeline.max_chunks}\")\n",
    "    print(f\"Temperature: {rag_pipeline.temperature}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing RAG pipeline: {str(e)}\")\n",
    "    print(\"Please check your Groq API key in the environment or .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load or Build Knowledge Base from Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's in the data directory\n",
    "data_path = \"./data\"\n",
    "print(f\"Contents of {data_path}:\")\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    for item in os.listdir(data_path):\n",
    "        item_path = os.path.join(data_path, item)\n",
    "        size = os.path.getsize(item_path) if os.path.isfile(item_path) else \"DIR\"\n",
    "        print(f\"  - {item} ({size} bytes)\" if isinstance(size, int) else f\"  - {item} ({size})\")\nelse:\n",
    "    print(\"Data directory not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize from documents or load existing vector store\n",
    "print(\"Setting up Knowledge Base...\")\n",
    "\n",
    "# Check if vector store already exists\n",
    "if not rag_pipeline.vector_store.load_vector_store():\n",
    "    print(\"No existing vector store found. Building from documents...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize from documents in data directory\n",
    "        success = rag_pipeline.initialize_from_documents(data_path)\n",
    "        \n",
    "        if success:\n",
    "            print(\"Knowledge base built successfully!\")\n",
    "        else:\n",
    "            print(\"Failed to build knowledge base\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error building knowledge base: {str(e)}\")\nelse:\n",
    "    print(\"Existing vector store loaded successfully!\")\n",
    "\n",
    "# Display knowledge base statistics\n",
    "try:\n",
    "    kb_stats = rag_pipeline.vector_store.get_stats()\n",
    "    print(\"\\nKnowledge Base Statistics:\")\n",
    "    for key, value in kb_stats.items():\n",
    "        print(f\"  {key}: {value}\")\nexcept Exception as e:\n",
    "    print(f\"Could not get knowledge base stats: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Store Testing & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries based on your document content\n",
    "# These are generic queries that should work with most document types\n",
    "test_queries = [\n",
    "    \"what is this about\",\n",
    "    \"main topic\",\n",
    "    \"key information\",\n",
    "    \"important details\",\n",
    "    \"summary\",\n",
    "    \"overview\",\n",
    "    \"purpose\",\n",
    "    \"definition\",\n",
    "    \"explanation\",\n",
    "    \"how it works\"\n",
    "]\n",
    "\n",
    "print(\"Testing Vector Store Similarity Search...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "search_results = {}\n",
    "all_scores = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[{i}/{len(test_queries)}] Query: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        results = rag_pipeline.vector_store.similarity_search(query, k=3)\n",
    "        search_results[query] = results\n",
    "        \n",
    "        if results:\n",
    "            scores = [result['score'] for result in results]\n",
    "            all_scores.extend(scores)\n",
    "            \n",
    "            print(f\"Found {len(results)} results\")\n",
    "            for j, result in enumerate(results, 1):\n",
    "                content_preview = result['content'][:100].replace('\\n', ' ')\n",
    "                print(f\"  {j}. Score: {result['score']:.3f}\")\n",
    "                print(f\"     Content: {content_preview}...\")\n",
    "                if 'metadata' in result and result['metadata']:\n",
    "                    print(f\"     Metadata: {result['metadata']}\")\n",
    "        else:\n",
    "            print(\"No results found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        search_results[query] = []\n",
    "\n",
    "print(f\"\\nSearch Results Summary:\")\n",
    "print(f\"  Total queries tested: {len(test_queries)}\")\n",
    "print(f\"  Queries with results: {len([q for q, r in search_results.items() if r])}\")\n",
    "if all_scores:\n",
    "    print(f\"  Average similarity score: {np.mean(all_scores):.3f}\")\n",
    "    print(f\"  Score range: {np.min(all_scores):.3f} - {np.max(all_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize search results\n",
    "if all_scores:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('Vector Store Search Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Score distribution\n",
    "    axes[0].hist(all_scores, bins=15, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('Similarity Score Distribution')\n",
    "    axes[0].set_xlabel('Similarity Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].axvline(np.mean(all_scores), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(all_scores):.3f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # 2. Results per query\n",
    "    result_counts = [len(results) for results in search_results.values()]\n",
    "    query_names = [q[:15] + '...' if len(q) > 15 else q for q in search_results.keys()]\n",
    "    \n",
    "    axes[1].bar(range(len(result_counts)), result_counts)\n",
    "    axes[1].set_title('Results per Query')\n",
    "    axes[1].set_xlabel('Query')\n",
    "    axes[1].set_ylabel('Number of Results')\n",
    "    axes[1].set_xticks(range(len(query_names)))\n",
    "    axes[1].set_xticklabels(query_names, rotation=45, ha='right')\n",
    "    \n",
    "    # 3. Score statistics box plot\n",
    "    query_scores = []\n",
    "    valid_queries = []\n",
    "    for query, results in search_results.items():\n",
    "        if results:\n",
    "            scores = [r['score'] for r in results]\n",
    "            query_scores.append(scores)\n",
    "            valid_queries.append(query[:10] + '...' if len(query) > 10 else query)\n",
    "    \n",
    "    if query_scores:\n",
    "        axes[2].boxplot(query_scores, labels=valid_queries)\n",
    "        axes[2].set_title('Score Distribution by Query')\n",
    "        axes[2].set_xlabel('Query')\n",
    "        axes[2].set_ylabel('Similarity Score')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No search results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Pipeline End-to-End Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation questions\n",
    "evaluation_questions = [\n",
    "    {\n",
    "        \"question\": \"What is the main topic of this document?\",\n",
    "        \"category\": \"Overview\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you summarize the key points?\",\n",
    "        \"category\": \"Summary\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the most important details mentioned?\",\n",
    "        \"category\": \"Details\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does this information relate to practical applications?\",\n",
    "        \"category\": \"Application\",\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What conclusions can be drawn from this information?\",\n",
    "        \"category\": \"Analysis\",\n",
    "        \"difficulty\": \"hard\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Running RAG Pipeline Evaluation with {len(evaluation_questions)} questions...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "evaluation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "for i, test_case in enumerate(evaluation_questions, 1):\n",
    "    question = test_case['question']\n",
    "    print(f\"\\n[{i}/{len(evaluation_questions)}] {test_case['category']} ({test_case['difficulty']})\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Measure retrieval time\n",
    "        retrieval_start = time.time()\n",
    "        context_chunks = rag_pipeline.retrieve_context(question)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        \n",
    "        # Measure generation time\n",
    "        generation_start = time.time()\n",
    "        context = rag_pipeline.format_context(context_chunks)\n",
    "        answer = rag_pipeline.generate_response(question, context)\n",
    "        generation_time = time.time() - generation_start\n",
    "        \n",
    "        total_time = retrieval_time + generation_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_retrieval_score = np.mean([chunk['score'] for chunk in context_chunks]) if context_chunks else 0\n",
    "        num_retrieved = len(context_chunks)\n",
    "        answer_length = len(answer)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'category': test_case['category'],\n",
    "            'difficulty': test_case['difficulty'],\n",
    "            'answer': answer,\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'generation_time': generation_time,\n",
    "            'total_time': total_time,\n",
    "            'avg_retrieval_score': avg_retrieval_score,\n",
    "            'num_retrieved': num_retrieved,\n",
    "            'answer_length': answer_length\n",
    "        }\n",
    "        \n",
    "        evaluation_results.append(result)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"Answer generated successfully!\")\n",
    "        print(f\"Metrics:\")\n",
    "        print(f\"   Retrieval time: {retrieval_time:.3f}s\")\n",
    "        print(f\"   Generation time: {generation_time:.3f}s\")\n",
    "        print(f\"   Total time: {total_time:.3f}s\")\n",
    "        print(f\"   Retrieved chunks: {num_retrieved}\")\n",
    "        print(f\"   Avg retrieval score: {avg_retrieval_score:.3f}\")\n",
    "        print(f\"   Answer length: {answer_length} chars\")\n",
    "        print(f\"\\nAnswer Preview:\")\n",
    "        print(f\"   {answer[:200]}...\" if len(answer) > 200 else f\"   {answer}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {str(e)}\")\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'category': test_case['category'],\n",
    "            'difficulty': test_case['difficulty'],\n",
    "            'error': str(e)\n",
    "        }\n",
    "        evaluation_results.append(result)\n",
    "\n",
    "print(f\"\\nEvaluation completed! Processed {len(evaluation_results)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "df_results = pd.DataFrame([r for r in evaluation_results if 'error' not in r])\n",
    "\n",
    "if not df_results.empty:\n",
    "    print(\"RAG Pipeline Performance Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(f\"Successful evaluations: {len(df_results)}/{len(evaluation_results)}\")\n",
    "    print(f\"\\nTiming Metrics:\")\n",
    "    print(f\"   Average retrieval time: {df_results['retrieval_time'].mean():.3f}s\")\n",
    "    print(f\"   Average generation time: {df_results['generation_time'].mean():.3f}s\")\n",
    "    print(f\"   Average total time: {df_results['total_time'].mean():.3f}s\")\n",
    "    \n",
    "    print(f\"\\nQuality Metrics:\")\n",
    "    print(f\"   Average retrieval score: {df_results['avg_retrieval_score'].mean():.3f}\")\n",
    "    print(f\"   Average chunks retrieved: {df_results['num_retrieved'].mean():.1f}\")\n",
    "    print(f\"   Average answer length: {df_results['answer_length'].mean():.0f} characters\")\n",
    "    \n",
    "    # Performance by category and difficulty\n",
    "    print(f\"\\nPerformance by Category:\")\n",
    "    category_stats = df_results.groupby('category').agg({\n",
    "        'total_time': 'mean',\n",
    "        'avg_retrieval_score': 'mean',\n",
    "        'answer_length': 'mean'\n",
    "    }).round(3)\n",
    "    print(category_stats)\n",
    "    \n",
    "    print(f\"\\nPerformance by Difficulty:\")\n",
    "    difficulty_stats = df_results.groupby('difficulty').agg({\n",
    "        'total_time': 'mean',\n",
    "        'avg_retrieval_score': 'mean',\n",
    "        'answer_length': 'mean'\n",
    "    }).round(3)\n",
    "    print(difficulty_stats)\n",
    "\nelse:\n",
    "    print(\"No successful evaluations to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "if not df_results.empty:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('RAG Pipeline Comprehensive Performance Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Response time breakdown\n",
    "    time_data = df_results[['retrieval_time', 'generation_time']].mean()\n",
    "    axes[0, 0].pie(time_data.values, labels=time_data.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Average Response Time Breakdown')\n",
    "    \n",
    "    # 2. Total time distribution\n",
    "    axes[0, 1].hist(df_results['total_time'], bins=10, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_title('Total Response Time Distribution')\n",
    "    axes[0, 1].set_xlabel('Total Time (seconds)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].axvline(df_results['total_time'].mean(), color='red', linestyle='--',\n",
    "                      label=f'Mean: {df_results[\"total_time\"].mean():.3f}s')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Performance by difficulty\n",
    "    if 'difficulty' in df_results.columns and len(df_results['difficulty'].unique()) > 1:\n",
    "        difficulty_times = [df_results[df_results['difficulty'] == d]['total_time'].values \n",
    "                           for d in ['easy', 'medium', 'hard'] \n",
    "                           if d in df_results['difficulty'].values]\n",
    "        difficulty_labels = [d for d in ['easy', 'medium', 'hard'] \n",
    "                            if d in df_results['difficulty'].values]\n",
    "        \n",
    "        if difficulty_times:\n",
    "            axes[0, 2].boxplot(difficulty_times, labels=difficulty_labels)\n",
    "            axes[0, 2].set_title('Response Time by Difficulty')\n",
    "            axes[0, 2].set_xlabel('Difficulty Level')\n",
    "            axes[0, 2].set_ylabel('Total Time (seconds)')\n",
    "    else:\n",
    "        axes[0, 2].text(0.5, 0.5, 'Insufficient difficulty\\nvariability', \n",
    "                       ha='center', va='center', transform=axes[0, 2].transAxes)\n",
    "        axes[0, 2].set_title('Response Time by Difficulty')\n",
    "    \n",
    "    # 4. Retrieval score distribution\n",
    "    axes[1, 0].hist(df_results['avg_retrieval_score'], bins=10, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Retrieval Score Distribution')\n",
    "    axes[1, 0].set_xlabel('Average Retrieval Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].axvline(df_results['avg_retrieval_score'].mean(), color='red', linestyle='--',\n",
    "                      label=f'Mean: {df_results[\"avg_retrieval_score\"].mean():.3f}')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 5. Answer length vs retrieval score\n",
    "    axes[1, 1].scatter(df_results['answer_length'], df_results['avg_retrieval_score'], \n",
    "                      alpha=0.7, s=100)\n",
    "    axes[1, 1].set_title('Answer Length vs Retrieval Quality')\n",
    "    axes[1, 1].set_xlabel('Answer Length (characters)')\n",
    "    axes[1, 1].set_ylabel('Average Retrieval Score')\n",
    "    \n",
    "    # 6. Performance by category (if multiple categories exist)\n",
    "    if len(df_results['category'].unique()) > 1:\n",
    "        category_means = df_results.groupby('category')['total_time'].mean()\n",
    "        axes[1, 2].bar(category_means.index, category_means.values)\n",
    "        axes[1, 2].set_title('Average Response Time by Category')\n",
    "        axes[1, 2].set_xlabel('Category')\n",
    "        axes[1, 2].set_ylabel('Average Time (seconds)')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        # Show overall metrics instead\n",
    "        metrics = ['Retrieval\\nTime', 'Generation\\nTime', 'Total\\nTime']\n",
    "        values = [df_results['retrieval_time'].mean(), \n",
    "                 df_results['generation_time'].mean(),\n",
    "                 df_results['total_time'].mean()]\n",
    "        axes[1, 2].bar(metrics, values)\n",
    "        axes[1, 2].set_title('Average Time Metrics')\n",
    "        axes[1, 2].set_ylabel('Time (seconds)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \nelse:\n",
    "    print(\"No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Optimization Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different parameter configurations\n",
    "print(\"Parameter Optimization Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Different configurations to test\n",
    "parameter_configs = [\n",
    "    {'max_chunks': 3, 'temperature': 0.0, 'name': 'Conservative'},\n",
    "    {'max_chunks': 5, 'temperature': 0.1, 'name': 'Balanced'}, # Current setting\n",
    "    {'max_chunks': 7, 'temperature': 0.3, 'name': 'Creative'},\n",
    "    {'max_chunks': 10, 'temperature': 0.0, 'name': 'Comprehensive'}\n",
    "]\n",
    "\n",
    "# Use first two questions for quick testing\n",
    "test_question = \"What is the main topic of this document?\"\n",
    "\n",
    "param_results = []\n",
    "\n",
    "for i, config in enumerate(parameter_configs, 1):\n",
    "    print(f\"\\n[{i}/{len(parameter_configs)}] Testing {config['name']} Configuration\")\n",
    "    print(f\"   max_chunks: {config['max_chunks']}, temperature: {config['temperature']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Temporarily change pipeline parameters\n",
    "        original_chunks = rag_pipeline.max_chunks\n",
    "        original_temp = rag_pipeline.temperature\n",
    "        \n",
    "        rag_pipeline.max_chunks = config['max_chunks']\n",
    "        rag_pipeline.temperature = config['temperature']\n",
    "        \n",
    "        # Test the configuration\n",
    "        start_time = time.time()\n",
    "        \n",
    "        context_chunks = rag_pipeline.retrieve_context(test_question)\n",
    "        context = rag_pipeline.format_context(context_chunks)\n",
    "        answer = rag_pipeline.generate_response(test_question, context)\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_score = np.mean([chunk['score'] for chunk in context_chunks]) if context_chunks else 0\n",
    "        \n",
    "        result = {\n",
    "            'config_name': config['name'],\n",
    "            'max_chunks': config['max_chunks'],\n",
    "            'temperature': config['temperature'],\n",
    "            'response_time': response_time,\n",
    "            'avg_retrieval_score': avg_score,\n",
    "            'chunks_retrieved': len(context_chunks),\n",
    "            'answer_length': len(answer),\n",
    "            'answer_preview': answer[:150] + '...' if len(answer) > 150 else answer\n",
    "        }\n",
    "        \n",
    "        param_results.append(result)\n",
    "        \n",
    "        print(f\"Response time: {response_time:.3f}s\")\n",
    "        print(f\"   Retrieval score: {avg_score:.3f}\")\n",
    "        print(f\"   Chunks used: {len(context_chunks)}\")\n",
    "        print(f\"   Answer length: {len(answer)} chars\")\n",
    "        print(f\"   Preview: {result['answer_preview']}\")\n",
    "        \n",
    "        # Restore original parameters\n",
    "        rag_pipeline.max_chunks = original_chunks\n",
    "        rag_pipeline.temperature = original_temp\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing configuration: {str(e)}\")\n",
    "        # Restore original parameters\n",
    "        rag_pipeline.max_chunks = original_chunks\n",
    "        rag_pipeline.temperature = original_temp\n",
    "\n",
    "print(\"\\nParameter Optimization Results Summary:\")\n",
    "if param_results:\n",
    "    param_df = pd.DataFrame(param_results)\n",
    "    print(param_df[['config_name', 'response_time', 'avg_retrieval_score', 'chunks_retrieved', 'answer_length']].round(3))\nelse:\n",
    "    print(\"No parameter test results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Comprehensive Test Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive test report\n",
    "print(\"Generating Comprehensive Test Report...\")\n",
    "\n",
    "test_report = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"test_environment\": {\n",
    "        \"model_name\": rag_pipeline.model_name,\n",
    "        \"api_provider\": \"Groq\",\n",
    "        \"data_directory\": data_path,\n",
    "        \"vector_store_path\": rag_pipeline.vector_store.vector_db_path\n",
    "    },\n",
    "    \"knowledge_base_stats\": kb_stats if 'kb_stats' in locals() else {},\n",
    "    \"vector_search_results\": {\n",
    "        \"total_queries_tested\": len(test_queries),\n",
    "        \"queries_with_results\": len([q for q, r in search_results.items() if r]),\n",
    "        \"average_similarity_score\": np.mean(all_scores) if all_scores else 0,\n",
    "        \"score_statistics\": {\n",
    "            \"min\": float(np.min(all_scores)) if all_scores else 0,\n",
    "            \"max\": float(np.max(all_scores)) if all_scores else 0,\n",
    "            \"mean\": float(np.mean(all_scores)) if all_scores else 0,\n",
    "            \"std\": float(np.std(all_scores)) if all_scores else 0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add RAG pipeline results if available\n",
    "if not df_results.empty:\n",
    "    test_report[\"rag_pipeline_performance\"] = {\n",
    "        \"total_questions_tested\": len(evaluation_questions),\n",
    "        \"successful_evaluations\": len(df_results),\n",
    "        \"failed_evaluations\": len(evaluation_questions) - len(df_results),\n",
    "        \"timing_metrics\": {\n",
    "            \"avg_retrieval_time_seconds\": float(df_results['retrieval_time'].mean()),\n",
    "            \"avg_generation_time_seconds\": float(df_results['generation_time'].mean()),\n",
    "            \"avg_total_time_seconds\": float(df_results['total_time'].mean()),\n",
    "            \"min_total_time_seconds\": float(df_results['total_time'].min()),\n",
    "            \"max_total_time_seconds\": float(df_results['total_time'].max())\n",
    "        },\n",
    "        \"quality_metrics\": {\n",
    "            \"avg_retrieval_score\": float(df_results['avg_retrieval_score'].mean()),\n",
    "            \"avg_chunks_retrieved\": float(df_results['num_retrieved'].mean()),\n",
    "            \"avg_answer_length_chars\": float(df_results['answer_length'].mean())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Performance by category\n",
    "    if len(df_results['category'].unique()) > 1:\n",
    "        test_report[\"rag_pipeline_performance\"][\"category_performance\"] = {}\n",
    "        for category in df_results['category'].unique():\n",
    "            cat_data = df_results[df_results['category'] == category]\n",
    "            test_report[\"rag_pipeline_performance\"][\"category_performance\"][category] = {\n",
    "                \"count\": len(cat_data),\n",
    "                \"avg_time_seconds\": float(cat_data['total_time'].mean()),\n",
    "                \"avg_retrieval_score\": float(cat_data['avg_retrieval_score'].mean())\n",
    "            }\n",
    "\n",
    "# Add parameter optimization results\n",
    "if param_results:\n",
    "    test_report[\"parameter_optimization\"] = {\n",
    "        \"configurations_tested\": len(param_results),\n",
    "        \"results\": param_results\n",
    "    }\n",
    "\n",
    "# Create results directory\n",
    "results_dir = \"./test_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save comprehensive report\n",
    "report_file = os.path.join(results_dir, f\"rag_comprehensive_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(test_report, f, indent=2, default=str)\n",
    "\n",
    "# Save detailed results if available\n",
    "if not df_results.empty:\n",
    "    csv_file = os.path.join(results_dir, f\"rag_evaluation_details_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    df_results.to_csv(csv_file, index=False)\n",
    "    print(f\"Detailed results saved: {csv_file}\")\n",
    "\n",
    "print(f\"Comprehensive test report saved: {report_file}\")\n",
    "print(\"\\nFinal Test Summary:\")\n",
    "print(f\"   Vector Store Tests: {test_report['vector_search_results']['queries_with_results']}/{test_report['vector_search_results']['total_queries_tested']} successful\")\n",
    "if 'rag_pipeline_performance' in test_report:\n",
    "    print(f\"   RAG Pipeline Tests: {test_report['rag_pipeline_performance']['successful_evaluations']}/{test_report['rag_pipeline_performance']['total_questions_tested']} successful\")\n",
    "    print(f\"   Average Response Time: {test_report['rag_pipeline_performance']['timing_metrics']['avg_total_time_seconds']:.3f}s\")\nif 'parameter_optimization' in test_report:\n",
    "    print(f\"   Parameter Configs Tested: {test_report['parameter_optimization']['configurations_tested']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on test results\n",
    "print(\"RAG Pipeline Testing Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Analyze results and generate recommendations\n",
    "if not df_results.empty:\n",
    "    avg_time = df_results['total_time'].mean()\n",
    "    avg_score = df_results['avg_retrieval_score'].mean()\n",
    "    \n",
    "    print(f\"\\nKey Performance Metrics:\")\n",
    "    print(f\"   Average Response Time: {avg_time:.3f}s\")\n",
    "    print(f\"   Average Retrieval Score: {avg_score:.3f}\")\n",
    "    print(f\"   Success Rate: {len(df_results)}/{len(evaluation_questions)} ({len(df_results)/len(evaluation_questions)*100:.1f}%)\")\n",
    "    \n",
    "    # Performance recommendations\n",
    "    if avg_time > 3.0:\n",
    "        recommendations.append(\"Consider optimizing for speed - response time is above 3s\")\n",
    "    elif avg_time < 1.0:\n",
    "        recommendations.append(\"Excellent response time performance - under 1s average\")\n",
    "    \n",
    "    # Quality recommendations\n",
    "    if avg_score < 0.7:\n",
    "        recommendations.append(\"Low retrieval scores - consider improving document chunking or preprocessing\")\n",
    "    elif avg_score > 0.85:\n",
    "        recommendations.append(\"Excellent retrieval quality - scores above 0.85\")\n",
    "    \n",
    "    # Vector store recommendations\n",
    "    if all_scores and np.mean(all_scores) < 0.6:\n",
    "        recommendations.append(\"Consider adding more diverse content to improve retrieval coverage\")\n",
    "\n",
    "# General recommendations\n",
    "recommendations.extend([\n",
    "    \"Test with more diverse question types specific to your domain\",\n",
    "    \"Implement user feedback collection for continuous improvement\",\n",
    "    \"Monitor performance in production with real user queries\",\n",
    "    \"Consider periodic retraining/updating of the knowledge base\",\n",
    "    \"Deploy with appropriate caching for frequently asked questions\"\n",
    "])\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"   1. Review test results and optimize parameters as needed\")\n",
    "print(f\"   2. Test with domain-specific questions relevant to your use case\")\n",
    "print(f\"   3. Consider A/B testing different Groq models (llama-3.1-70b-versatile for better quality)\")\n",
    "print(f\"   4. Implement monitoring and logging in your production deployment\")\n",
    "print(f\"   5. Set up automated testing pipeline for continuous quality assurance\")\n",
    "\n",
    "print(f\"\\nYour RAG pipeline is ready for deployment! All test files are saved in the test_results directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
